{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Naive Bayes\n",
    "***\n",
    "\n",
    "<img src=\"files/figs/bayes.jpg\",width=1201,height=50> \n",
    "\n",
    "<!---\n",
    "![my_image](files/figs/bayes.jpg)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prob1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Bayes Law and The Monte Hall Problem \n",
    "***\n",
    "\n",
    "\n",
    ">Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\" Is it to your advantage to switch your choice?\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Monty_open_door.svg/2000px-Monty_open_door.svg.png\",width=801,height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: What does your intuition say?  Is it in your best interest to switch? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B**: Use Bayes Rule to show that your probability of winning when switching is 2/3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Problem 1 Answers]](#prob1ans)\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prob2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Naive Bayes on Symbols\n",
    "***\n",
    "\n",
    "> This problem was adopted from [Naive Bayes and Text Classification I: Introduction and Theory](https://arxiv.org/abs/1410.5329) by Sebastian Raschka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following training set of 12 symbols which have been labeled as either + or -: \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"files/figs/shapes.png?raw=true\"; width=500>\n",
    "\n",
    "<!---\n",
    "![](files/figs/shapes.png?raw=true)\n",
    "-->\n",
    "\n",
    "Answer the following questions: \n",
    "\n",
    "\n",
    "**A**: What are the general features associated with each training example? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we'll use Naive Bayes to classify the following test example: \n",
    "\n",
    "<img src=\"files/figs/bluesquare.png\"; width=200>\n",
    "\n",
    "OK, so this symbol actually appears in the training set, but let's pretend that it doesn't.  \n",
    "\n",
    "The decision rule can be defined as \n",
    "\n",
    ">Classify ${\\bf x}$ as + if <br>\n",
    ">$p(+ ~|~ {\\bf x} = [blue,~ square]) \\geq p(- ~|~ {\\bf x} = [blue, ~square])$ <br>\n",
    ">else classify sample as -\n",
    "\n",
    "**B**: What are the Maximum Likelihood Estimates of the priors $p(+)$ and $p(-)$? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C**: Identify and compute estimates of the class-conditional probabilities required to predict the class of ${\\bf x} = [blue,~square]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D**: Using the estimates computed above, compute the **posterior** scores for each label, and find the Naive Bayes prediction of the label for ${\\bf x} = [blue,~square]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E**: If you haven't already, compute the class-conditional probabilities scores $\\hat{p}({\\bf x} = [blue,~square] ~|~ +)$ and $\\hat{p}({\\bf x} = [blue,~square] ~|~ -)$ under the Naive Bayes assumption.  How can you reconsile these values with the final prediction that would made? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Problem 2 Answers]](#prob2ans)\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prob3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Laplace Smoothing \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the same training set from Problem 2, but suppose we see the following test example: \n",
    "    \n",
    "<img src=\"figs/greencircle.png\"; width=200>\n",
    "\n",
    "Before you get too far into trying to predict the label of the green circle, look carefully at the training set.  Notice that there are no green shapes labeled - in the training set, so when we try to compute the class-conditional probability $p(green ~|~ -)$ we'll get a zero probability.  To fix this, you'll implement Laplace smoothing. Notice that this is a little different than the SPAM vs HAM example shown in the video.  We actually have two very different features in shapes and colors. We'll apply Laplace Smoothing to the shape and color class-conditional probabilities separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: What would the general formula for the estimate of $p(shape ~|~ class)$ with Laplace Smoothing look like for the given training set?  What is the *vocabulary* in the shape case?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B**: What would the general formula for the estimate of $p(color ~|~ class)$ with Laplace Smoothing look like for the given training set?  What is the *vocabulary* in the shape case?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C**: Predict the label for the green circle using the Laplaced smoothed class-conditional probability formulas.  Don't forget to apply Laplace Smoothing to the priors as well! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Problem 3 Answers]](#prob3ans)\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prob4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Unknown Features\n",
    "***\n",
    "\n",
    "Once again consider the training set from Problem 2, but suppose we see the following test example: \n",
    "    \n",
    "<img src=\"figs/yellowsquare.png\"; width=200>\n",
    "\n",
    "OK, this is a weird one.  Up until this point, we've never seen the color *yellow*, and thus don't include it in the color vocabulary.  One way that we could handle this is to add to the color vocabulary, and then recompute the the class-conditional probabilities with *yellow* included in the vocabulary. \n",
    "\n",
    "But what happens when on the next test example we see a *pink* circle (or worse, a triangle)? We'd rather not continue to modify our probability estimates whenever we see shape or color that we haven't see before.  One solution to this is to just assume we'll see weird things in the future and combine all of the posibilities into an UNK feature. If we do this, then our class-conditional probabilities become \n",
    "\n",
    "$$\n",
    "p(feature ~|~ class) = \\frac{\\#~instances~of~feature~in~class + 1}{\\#~total~symbols~in~class + |V| + 1}\n",
    "$$\n",
    "\n",
    "where here the vocabular $V$ is the same vocabular defined by the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Predict the label of the yellow square.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Problem 4 Answers]](#prob4ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prob1ans'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Bayes Law and The Monte Hall Problem \n",
    "***\n",
    "\n",
    "\n",
    ">Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\" Is it to your advantage to switch your choice?\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Monty_open_door.svg/2000px-Monty_open_door.svg.png\",width=800,height=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: What does your intuition say?  Is it in your best interest to switch? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  It is in your interest to switch.  It turns out that if you don't switch doors you have a 1/3 probability of winning the car.  If you do switch doors you have a 2/3 probability.  We'll prove this using Bayes rule in the next part, but if this seems sketchy to you, consider a modified game where there are 100 doors with 99 goats and one car.  After making your guess the host (knowing where the car is) opens 98 doors that have goats behind them.  Now does your intuition change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B**: Use Bayes Rule to show that your probability of winning when switching is 2/3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  Let's assume that you pick door number 1 and Monty opens door number 3.  The question then is whether you stick with door number 1 or switch to door number 2. \n",
    "\n",
    "Let $D_i$ be the event that the car is actually behind door $i$.  Let $H$ be the event that Monty opens door number 3.  We want to compute $p(D_1 ~|~ H)$ and $p(D_2 ~|~ H)$.  From Bayes Rule we have \n",
    "\n",
    "$$\n",
    "p(D_i ~|~ H) = \\frac{p(H ~|~ D_i) \\cdot p(D_i)}{p(H)}\n",
    "$$\n",
    "\n",
    "The prior (for any door) is \n",
    "\n",
    "$$\n",
    "p(D_i) = \\frac{1}{3}, ~ i = 1,\\ldots,3\n",
    "$$\n",
    "\n",
    "The likelihoods, given our assumptions, are as follows: \n",
    "\n",
    "$$\n",
    "p(H ~|~ D_1) = \\frac{1}{2} ~~~~~~~~~~\n",
    "p(H ~|~ D_2) = 1 ~~~~~~~~~~\n",
    "p(H ~|~ D_3) = 0 \n",
    "$$\n",
    "\n",
    "The marginal for $H$ we can compute using the sum rule of probability.  We have \n",
    "\n",
    "$$\n",
    "p(H) = \\sum_i p(H, D_i) = \\sum_i p(H ~|~ D_i)~p(D_i) = \\frac{1}{2}\\cdot\\frac{1}{3} + 1\\cdot\\frac{1}{3} + 0\\cdot\\frac{1}{3} = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "Plugging these numbers into Bayes Rule, we have\n",
    "\n",
    "$$\n",
    "p(D_1 ~|~ H) = \\frac{\\frac{1}{2} \\cdot \\frac{1}{3}}{\\frac{1}{2}} = \\frac{1}{3} ~~~~~~~~ \\textrm{and} ~~~~~~~~\n",
    "p(D_2 ~|~ H) = \\frac{1 \\cdot \\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n",
    "$$\n",
    "\n",
    "Therefore, to maximize your chance of winning the car, you should always switched doors when given the chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to Problem 1]](#prob1)\n",
    "\n",
    "<a id='prob2ans'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Naive Bayes on Symbols\n",
    "***\n",
    "\n",
    "> This problem was adopted from [Naive Bayes and Text Classification I: Introduction and Theory](https://arxiv.org/abs/1410.5329) by Sebastian Raschka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following training set of 12 symbols which have been labeled as either + or -: \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figs/shapes.png\"; width=500>\n",
    "\n",
    "Answer the following questions: \n",
    "\n",
    "\n",
    "**A**: What are the general features associated with each training example? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The two general types of features are **shape** and **color**.  For this particular training set, the observed features are *shape* $\\in$ {*square*, *circle*} and *color* $\\in$ {*red*, *blue*, *green*}. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we'll use Naive Bayes to classify the following test example: \n",
    "\n",
    "<img src=\"figs/bluesquare.png\"; width=200>\n",
    "\n",
    "OK, so this symbol actually appears in the training set, but let's pretend that it doesn't.  \n",
    "\n",
    "The decision rule can be defined as \n",
    "\n",
    ">Classify ${\\bf x}$ as + if <br>\n",
    ">$p(+ ~|~ {\\bf x} = [blue,~ square]) \\geq p(- ~|~ {\\bf x} = [blue, ~square])$ <br>\n",
    ">else classify sample as -\n",
    "\n",
    "**B**: What are the Maximum Likelihood Estimates of the priors $p(+)$ and $p(-)$? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: There are 12 symbols in the training set with 7 of them labeled + and 5 of them labeled -, so we have\n",
    "\n",
    "$$\n",
    "\\hat{p}(+) = \\frac{7}{12} ~~~ \\textrm{and} ~~~ \\hat{p}(-) = \\frac{5}{12} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C**: Identify and compute estimates of the class-conditional probabilities required to predict the class of ${\\bf x} = [blue,~square]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The class-conditional probabilities required to classify ${\\bf x} = [blue, ~square]$ are \n",
    "\n",
    "$$\n",
    "p(blue ~|~ +), ~~~~~ p(blue ~|~ -), ~~~~~ p(square ~|~ +), ~~~~~ p(square ~|~ -)\n",
    "$$\n",
    "\n",
    "From the training set, we have \n",
    "\n",
    "$$\n",
    "\\hat{p}(blue ~|~ +)= \\frac{3}{7}, ~~~~~ \\hat{p}(blue ~|~ -) = \\frac{3}{5}, ~~~~~ \\hat{p}(square ~|~ +)=\\frac{5}{7}, ~~~~~ \\hat{p}(square ~|~ -) = \\frac{3}{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D**: Using the estimates computed above, compute the **posterior** scores for each label, and find the Naive Bayes prediction of the label for ${\\bf x} = [blue,~square]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: We have \n",
    "\n",
    "$$\n",
    "\\hat{p}(+ ~|~ {\\bf x} = [blue,~square]) = \\hat{p}(blue ~|~ +) \\cdot \\hat{p}(square ~|~ +) \\cdot \\hat{p}(+) = \\frac{3}{7} \\cdot \\frac{5}{7} \\cdot \\frac{7}{12} = 0.18 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{p}(- ~|~ {\\bf x} = [blue,~square]) = \\hat{p}(blue ~|~ -) \\cdot \\hat{p}(square ~|~ -) \\cdot \\hat{p}(-) = \\frac{3}{5} \\cdot \\frac{3}{5} \\cdot \\frac{5}{12} = 0.15\n",
    "$$\n",
    "\n",
    "Since $0.18 > 0.15$,  our Naive Bayes classifier would classify ${\\bf x} = [blue, ~square]$ as + "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E**: If you haven't already, compute the class-conditional probabilities scores $\\hat{p}({\\bf x} = [blue,~square] ~|~ +)$ and $\\hat{p}({\\bf x} = [blue,~square] ~|~ -)$ under the Naive Bayes assumption.  How can you reconsile these values with the final prediction that would made? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The class-conditional probability scores under the Naive Bayes assumption are \n",
    "\n",
    "$$\n",
    "\\hat{p}({\\bf x} = [blue,~square] ~|~ +) = \\hat{p}(blue ~|~ +) \\cdot \\hat{p}(square ~|~ +)  = \\frac{3}{7} \\cdot \\frac{5}{7} = 0.31\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{p}({\\bf x} = [blue,~square] ~|~ -) = \\hat{p}(blue ~|~ -) \\cdot \\hat{p}(square ~|~ -) = \\frac{3}{5} \\cdot \\frac{3}{5} = 0.36\n",
    "$$\n",
    "\n",
    "The - label actually has a higher class-conditional probability for ${\\bf x}$ than the + label.  We ended up predicting the + label because the prior for + was larger than the prior for -.  This example demonstrates how the choice of prior can have a large influence on the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to Problem 2]](#prob2)\n",
    "\n",
    "<a id='prob3ans'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Laplace Smoothing \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the same training set from Problem 2, but suppose we see the following test example: \n",
    "    \n",
    "<img src=\"figs/greencircle.png\"; width=200>\n",
    "\n",
    "Before you get too far into trying to predict the label of the green circle, look carefully at the training set.  Notice that there are no green shapes labeled - in the training set, so when we try to compute the class-conditional probability $p(green ~|~ -)$ we'll get a zero probability.  To fix this, you'll implement Laplace smoothing. Notice that this is a little different than the SPAM vs HAM example shown in the video.  We actually have two very different features in shapes and colors. We'll apply Laplace Smoothing to the shape and color class-conditional probabilities separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: What would the general formula for the estimate of $p(shape ~|~ class)$ with Laplace Smoothing look like for the given training set?  What is the *vocabulary* in the shape case?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Since the training set only includes the shapes {*square*, *circle*}.  This is the vocabulary.  Since there are two things in the vocabulary, the estimate of $p(shape ~|~ class)$ becomes \n",
    "\n",
    "$$\n",
    "p(shape ~|~ class) = \\frac{\\#~instances~of~shape~in~class + 1}{\\#~total~symbols~in~class + 2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B**: What would the general formula for the estimate of $p(color ~|~ class)$ with Laplace Smoothing look like for the given training set?  What is the *vocabulary* in the shape case?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Since the training set only includes the colors {*red*, *green*, *blue*}.  This is the vocabulary.  Since there are three things in the vocabulary, the estimate of $p(color ~|~ class)$ becomes \n",
    "\n",
    "$$\n",
    "p(color ~|~ class) = \\frac{\\#~instances~of~color~in~class + 1}{\\#~total~symbols~in~class + 3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C**: Predict the label for the green circle using the Laplaced smoothed class-conditional probability formulas.  Don't forget to apply Laplace Smoothing to the priors as well! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The posterior scores under the Naive Bayes assumption are \n",
    "\n",
    "$$\n",
    "\\hat{p}(+ ~|~ {\\bf x} = [green,~circle]) = \\hat{p}(green ~|~ +) \\cdot \\hat{p}(circle ~|~ +) \\cdot \\hat{p}(+) = \\frac{2+1}{7+3} \\cdot \\frac{2+1}{7+2} \\cdot \\frac{7+1}{12+2}= 0.057\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{p}(- ~|~ {\\bf x} = [green,~circle]) = \\hat{p}(green ~|~ -) \\cdot \\hat{p}(circle ~|~ -) \\cdot \\hat{p}(-) = \\frac{0+1}{5+3} \\cdot \\frac{2+1}{5+2} \\cdot \\frac{5+1}{12+2}= 0.023\n",
    "$$\n",
    "\n",
    "Since $0.057 > 0.023$ we again predict + for the label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to Problem 3]](#prob3)\n",
    "\n",
    "<a id='prob4ans'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Unknown Features\n",
    "***\n",
    "\n",
    "Once again consider the training set from Problem 2, but suppose we see the following test example: \n",
    "    \n",
    "<img src=\"figs/yellowsquare.png\"; width=200>\n",
    "\n",
    "OK, this is a weird one.  Up until this point, we've never seen the color *yellow*, and thus don't include it in the color vocabulary.  One way that we could handle this is to add to the color vocabulary, and then recompute the the class-conditional probabilities with *yellow* included in the vocabulary. \n",
    "\n",
    "But what happens when on the next test example we see a *pink* circle (or worse, a triangle)? We'd rather not continue to modify our probability estimates whenever we see shape or color that we haven't see before.  One solution to this is to just assume we'll see weird things in the future and combine all of the posibilities into an UNK feature. If we do this, then our class-conditional probabilities become \n",
    "\n",
    "$$\n",
    "p(feature ~|~ class) = \\frac{\\#~instances~of~feature~in~class + 1}{\\#~total~symbols~in~class + |V| + 1}\n",
    "$$\n",
    "\n",
    "where here the vocabular $V$ is the same vocabular defined by the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Predict the label of the yellow square.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The posterior scores under the Naive Bayes assumption are \n",
    "\n",
    "$$\n",
    "\\hat{p}(+ ~|~ {\\bf x} = [yellow,~square]) = \\hat{p}(yellow ~|~ +) \\cdot \\hat{p}(square ~|~ +) \\cdot \\hat{p}(+) = \\frac{0+1}{7+3+1} \\cdot \\frac{5+1}{7+2+1} \\cdot \\frac{7+1}{12+2}= 0.031\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{p}(- ~|~ {\\bf x} = [yellow,~square]) = \\hat{p}(yellow ~|~ -) \\cdot \\hat{p}(square ~|~ -) \\cdot \\hat{p}(-) = \\frac{0+1}{5+3+1} \\cdot \\frac{3+1}{5+2+1} \\cdot \\frac{5+1}{12+2}= 0.024\n",
    "$$\n",
    "\n",
    "Since $0.031 > 0.024$ we again predict + for the label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Back to Problem 4]](#prob4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "### Helper Functions \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
